{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1-oRijAoUwpp2Qlz-Kl3k6hNlodWQyf87","timestamp":1675435070291}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"37puETfgRzzg"},"source":["# Data Preprocessing Tools"]},{"cell_type":"markdown","metadata":{"id":"EoRP98MpR-qj"},"source":["## Importing the libraries"]},{"cell_type":"code","source":["import numpy as np \n","# Array creation: NumPy provides functions for creating arrays of different shapes and sizes, \n","# including arrays filled with zeros, ones, constant values, and arrays created from existing data.\n","\n","# Array operations: NumPy supports a variety of array operations, such as element-wise addition, \n","# subtraction, multiplication, and division, as well as more complex operations \n","# such as matrix multiplication and element-wise functions like exponential and logarithm.\n","\n","import matplotlib.pyplot as plt \n","# pyplot is a module in the matplotlib library, which provides a convenient interface for plotting \n","# data in Python. pyplot provides functions for creating a variety of different types of plots, including line plots, scatter plots, bar plots, histograms, and more. Additionally, pyplot includes a number of functions for customizing plots, \n","# such as adding titles, labels, and legends, adjusting axis limits and scales, and controlling the appearance of lines, markers, and other elements.\n","\n","import pandas as pd \n","# pandas is a library in Python used for data analysis and manipulation.\n","\n","# Data import and export: pandas provides functions for reading and writing data from a variety of different file formats, including CSV, Excel, JSON, and SQL databases.\n","\n","# Data cleaning and transformation: pandas provides a rich set of functions for cleaning and transforming data, such as removing duplicates, handling missing values, and transforming data using operations such as grouping and pivoting.\n","\n","# Data aggregation and summarization: pandas provides functions for aggregating and summarizing data, such as computing means, medians, and other summary statistics.\n","\n","# Data visualization: pandas integrates well with the matplotlib library, providing functions for creating a variety of different types of plots and charts, such as line plots, bar plots, histograms, and scatter plots."],"metadata":{"id":"MVIyPTpNuUvL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RopL7tUZSQkT"},"source":["## Importing the dataset"]},{"cell_type":"code","source":["dataset = pd.read_csv('Data.csv')\n","# pd.read_csv is a function in the pandas library that reads data from a CSV (Comma-Separated Values) file and returns a DataFrame. \n","# A DataFrame is a two-dimensional data structure that can store heterogeneous data and is similar to a spreadsheet or a SQL table.\n","dataset.head(10)"],"metadata":{"id":"VCN8OPbw0ZZc","executionInfo":{"status":"ok","timestamp":1675770281911,"user_tz":-330,"elapsed":31,"user":{"displayName":"Shashank Burhade","userId":"02023106116626809453"}},"colab":{"base_uri":"https://localhost:8080/","height":363},"outputId":"f0645a7a-3c51-4eaf-81c5-7faeee25fb3b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   Country   Age   Salary Purchased\n","0   France  44.0  72000.0        No\n","1    Spain  27.0  48000.0       Yes\n","2  Germany  30.0  54000.0        No\n","3    Spain  38.0  61000.0        No\n","4  Germany  40.0      NaN       Yes\n","5   France  35.0  58000.0       Yes\n","6    Spain   NaN  52000.0        No\n","7   France  48.0  79000.0       Yes\n","8  Germany  50.0  83000.0        No\n","9   France  37.0  67000.0       Yes"],"text/html":["\n","  <div id=\"df-320c4309-e6de-4c65-b5f2-f63c883f98dd\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Country</th>\n","      <th>Age</th>\n","      <th>Salary</th>\n","      <th>Purchased</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>France</td>\n","      <td>44.0</td>\n","      <td>72000.0</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Spain</td>\n","      <td>27.0</td>\n","      <td>48000.0</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Germany</td>\n","      <td>30.0</td>\n","      <td>54000.0</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Spain</td>\n","      <td>38.0</td>\n","      <td>61000.0</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Germany</td>\n","      <td>40.0</td>\n","      <td>NaN</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>France</td>\n","      <td>35.0</td>\n","      <td>58000.0</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Spain</td>\n","      <td>NaN</td>\n","      <td>52000.0</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>France</td>\n","      <td>48.0</td>\n","      <td>79000.0</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Germany</td>\n","      <td>50.0</td>\n","      <td>83000.0</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>France</td>\n","      <td>37.0</td>\n","      <td>67000.0</td>\n","      <td>Yes</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-320c4309-e6de-4c65-b5f2-f63c883f98dd')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-320c4309-e6de-4c65-b5f2-f63c883f98dd button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-320c4309-e6de-4c65-b5f2-f63c883f98dd');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":32}]},{"cell_type":"markdown","source":["Splitting the data in independent and dependent features."],"metadata":{"id":"HLeBVp3UF65L"}},{"cell_type":"code","source":["X = dataset.iloc[:, :-1].values # Independent features\n","Y = dataset.iloc[:, -1].values # Dependent features"],"metadata":{"id":"7shu7hWHFFp5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X)"],"metadata":{"id":"7YnSuByAAa5N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675770371582,"user_tz":-330,"elapsed":9,"user":{"displayName":"Shashank Burhade","userId":"02023106116626809453"}},"outputId":"5c7cb8d1-6644-47b6-9421-efe3904a1755"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[['France' 44.0 72000.0]\n"," ['Spain' 27.0 48000.0]\n"," ['Germany' 30.0 54000.0]\n"," ['Spain' 38.0 61000.0]\n"," ['Germany' 40.0 nan]\n"," ['France' 35.0 58000.0]\n"," ['Spain' nan 52000.0]\n"," ['France' 48.0 79000.0]\n"," ['Germany' 50.0 83000.0]\n"," ['France' 37.0 67000.0]]\n"]}]},{"cell_type":"code","source":["print(Y)"],"metadata":{"id":"7qXFCN8YAcjn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675770376263,"user_tz":-330,"elapsed":858,"user":{"displayName":"Shashank Burhade","userId":"02023106116626809453"}},"outputId":"0e8b8020-f127-4106-8675-2e2e601025e8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"]}]},{"cell_type":"markdown","metadata":{"id":"nhfKXNxlSabC"},"source":["## Taking care of missing data"]},{"cell_type":"markdown","source":["1)One of the method we can use is deleting the whole row of missing data\n","\n","2)Scikit-learn is a free, open-source machine learning library for Python. It is used for data analysis, data mining, and data visualization. \n","\n","3)Scikit-learn provides algorithms for supervised and unsupervised learning, including classification, regression, clustering, and dimensionality reduction. \n","\n","3)It also includes tools for feature selection, \n","feature extraction, model selection, and evaluation."],"metadata":{"id":"DG1xgB4tvWIf"}},{"cell_type":"code","source":["from sklearn.impute import SimpleImputer\n","imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean') # In this step the object imputer is still not connected to the matrix of features.\n","\n","# The code imports the SimpleImputer class from the scikit-learn library's impute module. The imputer object is then created with the SimpleImputer constructor and is initialized with two parameters:\n","# missing_values: This parameter specifies which values to replace with the imputed value. In this case, the value np.nan is passed, which represents missing or undefined values in a NumPy array.\n","\n","# strategy: This parameter specifies the strategy to use for imputing missing values. In this case, the value 'mean' is passed, which indicates that the mean value of each column should be used to fill in missing values in that column.\n","# The imputer object can then be used to transform data by calling the 'fit' and transform methods on it.\n","\n","X[: , 1:3] = imputer.fit_transform(X[: , 1:3]) "],"metadata":{"id":"lMyX0b2rFArp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Awy5kJBjMd6j","executionInfo":{"status":"ok","timestamp":1675770383646,"user_tz":-330,"elapsed":16,"user":{"displayName":"Shashank Burhade","userId":"02023106116626809453"}},"outputId":"89c3c1d7-6e64-42e0-ded5-8e85fe539ef9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[['France' 44.0 72000.0]\n"," ['Spain' 27.0 48000.0]\n"," ['Germany' 30.0 54000.0]\n"," ['Spain' 38.0 61000.0]\n"," ['Germany' 40.0 63777.77777777778]\n"," ['France' 35.0 58000.0]\n"," ['Spain' 38.77777777777778 52000.0]\n"," ['France' 48.0 79000.0]\n"," ['Germany' 50.0 83000.0]\n"," ['France' 37.0 67000.0]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"CriG6VzVSjcK"},"source":["## Encoding categorical data"]},{"cell_type":"markdown","source":["In machine learning, encoding refers to the process of converting categorical data (such as strings or labels) into numerical data that can be processed by a model. This is necessary because many machine learning algorithms require that the inputs be represented as numbers, rather than text or other data types. There are several common encoding techniques, including one-hot encoding, label encoding, and ordinal encoding, each of which is suited to different types of data and use cases."],"metadata":{"id":"NdmKeJhFwBdT"}},{"cell_type":"markdown","metadata":{"id":"AhSpdQWeSsFh"},"source":["### Encoding the Independent Variable"]},{"cell_type":"markdown","source":["In this case, the list of transformers has only one element: the tuple ('encoder', OneHotEncoder(), [0]). The name of the transformer is 'encoder', and it's an instance of the OneHotEncoder class. The list of column indices is [0], which means that the OneHotEncoder will be applied only to the first column of the data.\n","\n","The argument 'remainder = 'passthrough'' means that the remaining columns that are not specified in the list of transformers will be passed through the ColumnTransformer unchanged. The 'passthrough' value is the default behavior of ColumnTransformer, so it is not necessary to include it in this example."],"metadata":{"id":"f3uGcAC4zGc1"}},{"cell_type":"code","source":["from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import OneHotEncoder\n","\n","# Create an instance of the ColumnTransformer class\n","ct = ColumnTransformer(transformers = [('encoder',OneHotEncoder(), [0])] , remainder = 'passthrough') # for 1 column\n","'''ct = ColumnTransformer(transformers = [('encoder',OneHotEncoder(), [0])], ('encoder',OneHotEncoder(), [1])] , remainder = 'passthrough')''' # for 1 or more column\n","\n","# Apply the one-hot encoding to the first column of X\n","X = np.array(ct.fit_transform(X))"],"metadata":{"id":"_W-sJeWB1bjB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The reason the transformed data is converted into a numpy array is that many machine learning algorithms in scikit-learn and other libraries expect the input data to be in numpy array format."],"metadata":{"id":"KyiJXKimbpjm"}},{"cell_type":"code","source":["print(X)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UjxgRmQp1kZI","executionInfo":{"status":"ok","timestamp":1675770400614,"user_tz":-330,"elapsed":561,"user":{"displayName":"Shashank Burhade","userId":"02023106116626809453"}},"outputId":"ce087d4d-96c1-4238-ee06-2e214d7c5127"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1.0 0.0 0.0 44.0 72000.0]\n"," [0.0 0.0 1.0 27.0 48000.0]\n"," [0.0 1.0 0.0 30.0 54000.0]\n"," [0.0 0.0 1.0 38.0 61000.0]\n"," [0.0 1.0 0.0 40.0 63777.77777777778]\n"," [1.0 0.0 0.0 35.0 58000.0]\n"," [0.0 0.0 1.0 38.77777777777778 52000.0]\n"," [1.0 0.0 0.0 48.0 79000.0]\n"," [0.0 1.0 0.0 50.0 83000.0]\n"," [1.0 0.0 0.0 37.0 67000.0]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"DXh8oVSITIc6"},"source":["### Encoding the Dependent Variable"]},{"cell_type":"code","source":["from sklearn.preprocessing import LabelEncoder\n","\n","le = LabelEncoder()\n","\n","Y = le.fit_transform(Y)"],"metadata":{"id":"4UvzBUxG151l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(Y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PpchqXPx65ue","executionInfo":{"status":"ok","timestamp":1675770407785,"user_tz":-330,"elapsed":16,"user":{"displayName":"Shashank Burhade","userId":"02023106116626809453"}},"outputId":"46796761-d2d3-4060-b7e3-b7ee6441fc58"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0 1 0 0 1 1 0 1 0 1]\n"]}]},{"cell_type":"markdown","source":["LabelEncoder is used to convert categorical variables with a limited number of categories into numerical values. It assigns each category a unique integer value, and converts the categorical variable into an integer representation. For example, if a categorical variable has three categories (A, B, C), the LabelEncoder would convert the categories into the integers 0, 1, and 2, respectively.\n","\n","OneHotEncoder, on the other hand, converts the categorical variable into a binary representation known as one-hot encoding. In one-hot encoding, each category is represented by a binary vector with as many elements as there are categories. The position of the 1 in the binary vector represents the category, and all other elements are 0. For example, if a categorical variable has three categories (A, B, C), the OneHotEncoder would convert the categories into the binary vectors [1, 0, 0], [0, 1, 0], and [0, 0, 1], respectively.\n","\n","In general, OneHotEncoder is preferred over LabelEncoder when the categorical variable has a large number of categories, or when there is no natural order to the categories. When the categorical variable has a small number of categories and there is a natural order to the categories, LabelEncoder can be used."],"metadata":{"id":"pGufK8xF6deU"}},{"cell_type":"markdown","metadata":{"id":"qb_vcgm3qZKW"},"source":["## Splitting the dataset into the Training set and Test set"]},{"cell_type":"markdown","source":["Feature selection should be done after train-test splitting to avoid leaking information from the test set into the training pipeline."],"metadata":{"id":"yZuhFdcVxb3g"}},{"cell_type":"code","source":["# splits a dataset into training and testing sets using the train_test_split function from the scikit-learn library:\n","from sklearn.model_selection import train_test_split # the function train_test_split returns x train, x test, y train, y test\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 1)\n"],"metadata":{"id":"82QJ6HWYxebl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X_train) # 8 taken randomly"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"scv9dpQoBOVK","executionInfo":{"status":"ok","timestamp":1675770415056,"user_tz":-330,"elapsed":11,"user":{"displayName":"Shashank Burhade","userId":"02023106116626809453"}},"outputId":"4cd59a06-91a3-4c7d-ce5e-bd971b21a41b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.0 0.0 1.0 38.77777777777778 52000.0]\n"," [0.0 1.0 0.0 40.0 63777.77777777778]\n"," [1.0 0.0 0.0 44.0 72000.0]\n"," [0.0 0.0 1.0 38.0 61000.0]\n"," [0.0 0.0 1.0 27.0 48000.0]\n"," [1.0 0.0 0.0 48.0 79000.0]\n"," [0.0 1.0 0.0 50.0 83000.0]\n"," [1.0 0.0 0.0 35.0 58000.0]]\n"]}]},{"cell_type":"code","source":["print(X_test) # rest 2 "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ylUcLHV0BRA3","executionInfo":{"status":"ok","timestamp":1675770418179,"user_tz":-330,"elapsed":18,"user":{"displayName":"Shashank Burhade","userId":"02023106116626809453"}},"outputId":"326f6730-c7bd-4b7b-8017-211da105168c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.0 1.0 0.0 30.0 54000.0]\n"," [1.0 0.0 0.0 37.0 67000.0]]\n"]}]},{"cell_type":"code","source":["print(Y_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yis2cOOxBTeF","executionInfo":{"status":"ok","timestamp":1675770421802,"user_tz":-330,"elapsed":14,"user":{"displayName":"Shashank Burhade","userId":"02023106116626809453"}},"outputId":"14d908b4-f7c1-4310-b6e6-0be1a1668ae7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0 1 0 0 1 1 0 1]\n"]}]},{"cell_type":"code","source":["print(Y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TlPwZJB_BXRS","executionInfo":{"status":"ok","timestamp":1675770424537,"user_tz":-330,"elapsed":7,"user":{"displayName":"Shashank Burhade","userId":"02023106116626809453"}},"outputId":"0b5f361e-faa7-44cc-b696-4e0ac371f1f9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0 1]\n"]}]},{"cell_type":"markdown","source":["X_train, X_test, Y_train, and Y_test are likely terms used in a machine learning context.\n","\n","    X_train is a set of training data for input features, used to train a machine learning model.\n","    X_test is a set of test data for input features, used to evaluate the performance of a trained model.\n","    Y_train is a set of training data for the target variable(s), corresponding to the X_train input features.\n","    Y_test is a set of test data for the target variable(s), corresponding to the X_test input features.\n","\n","These datasets are usually split from a larger dataset to create a training set and a test set, so that the model can be trained on the training set and evaluated on the test set to estimate its generalization performance.\n","\n","***The train set is a dataset used to train a machine learning model.*** The model learns the relationship between the input features (represented by X_train) and the target variable(s) (represented by Y_train) in the train set.\n","\n","The test set is a dataset used to evaluate the performance of a trained machine learning model. The model is applied to the input features in the test set (represented by X_test), and its predictions for the target variable(s) are compared to the actual target values in the test set (represented by Y_test). The performance of the model is then evaluated based on the accuracy of its predictions on the test set."],"metadata":{"id":"mdZESedF1QmY"}},{"cell_type":"markdown","metadata":{"id":"TpGqbS4TqkIR"},"source":["## Feature Scaling"]},{"cell_type":"markdown","source":["feature scaling can be applied to both independent and dependent features, although it is more commonly applied to independent features. The goal of feature scaling is to transform the features into a common range, so that no feature has an undue influence on the model due to its scale. This helps to ensure that the model is not biased towards any particular feature."],"metadata":{"id":"kR44r2dopnrJ"}},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","sc = StandardScaler()\n","X_train[:, 3:] = sc.fit_transform(X_train[:, 3:])\n","X_test[:, 3:] = sc.transform(X_test[:, 3:])"],"metadata":{"id":"m_LldwJrMAPV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lySWD_QFQeLY","executionInfo":{"status":"ok","timestamp":1675770432779,"user_tz":-330,"elapsed":24,"user":{"displayName":"Shashank Burhade","userId":"02023106116626809453"}},"outputId":"378f5591-124a-46bb-a55e-cf85cfb15ca4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n"," [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n"," [1.0 0.0 0.0 0.566708506533324 0.633562432710455]\n"," [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n"," [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n"," [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n"," [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n"," [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n"]}]},{"cell_type":"code","source":["print(X_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kb7y6e2oRUFj","executionInfo":{"status":"ok","timestamp":1675770438022,"user_tz":-330,"elapsed":777,"user":{"displayName":"Shashank Burhade","userId":"02023106116626809453"}},"outputId":"fb8e9b1b-07ff-4362-8b11-5355e2c1ef97"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.0 1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n"," [1.0 0.0 0.0 -0.44973664397484414 0.2056403393225306]]\n"]}]},{"cell_type":"markdown","source":["do we need to apply standardization to dummy variables during feature scaling\n","\n","No, standardization (subtracting the mean and dividing by the standard deviation) is not typically applied to dummy variables in feature scaling.\n","\n","Dummy variables are binary or categorical variables that are used to represent the presence or absence of a certain category or attribute. In machine learning, they are often used to represent categorical features that can take on multiple values. Since dummy variables only take on values of 0 or 1, standardization is not applicable to these variables.\n","\n","standardization provides values betwen -3 to 3, so need for doing feature scaling on dummy variables"],"metadata":{"id":"_pn7_JE0M_cz"}}]}